# -*- coding: utf-8 -*-
# Osaka Kojo Furiwake Bot â€” BOM + é…è»Šè¡¨ + USB Upload version:
# - Uses bom_downloader.py to fetch:
#       BOM  -> ./BOM/<æ—¥ä»˜>/
#       é…è»Šè¡¨ -> ./é…è»Šè¡¨/
# - Reads é…è»Š(äºˆå®š) from ./é…è»Šè¡¨
# - Matches against BOM under ./BOM/<æ—¥ä»˜>/
# - strict PDF=1 rule, Excel=floors rule (é›†åˆä½å®… = floors multiple OK)
# - furiwake to ./â–½USB/<å·è»Š>/(å‰²ä»˜å›³, excels)
# - AFTER furiwake: uploads â–½USB under
#       https://nskkogyo.sharepoint.com/sites/yanase/Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}
# - results saved under ./Results, with è³‡æ–™UP + OK/NG coloring
# - No SharePoint upload of BOM/results, only USB folder

import os, re, shutil, logging, unicodedata, math, base64
from datetime import datetime
from pathlib import Path, PurePosixPath

import requests
import jaconv
import pandas as pd
from openpyxl import load_workbook, Workbook
from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
from openpyxl.utils import get_column_letter

import tkinter as tk
import customtkinter as ctk
from tkcalendar import Calendar

# ---------- Nasiwak / Logging ----------
from Nasiwak import *  # Bot_Update, etc.
from logging_setup import setup_logging
from bom_downloader import (
    download_factory_bom_for_date,
    download_osaka_haisha_for_date,
    download_tochigi_haisha_for_date,
    upload_usb_to_tochigi_date,
)
from config_access_token import get_access_token  # Graph access token
import argparse

setup_logging()
logger = logging.getLogger(__name__)

GRAPH_BASE_URL = "https://graph.microsoft.com/v1.0"

# ----------------- Normalisation helpers -----------------
JA_TRANS = str.maketrans({
    "ï¼ˆ": "(", "ï¼‰": ")", "ï¼": "-", "ãƒ¼": "-", "ãƒ»": "ï½¥", "ï¼¦": "F", "éš": "F", "ã€€": " "
})

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--å·¥å ´",
        choices=["å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿", "æ ƒæœ¨å·¥å ´"],
        required=True,
    )
    parser.add_argument(
        "--æ—¥ä»˜",
        required=True,
    )
    return parser.parse_args()

def ja_norm_strict(s: str) -> str:
    """For exact equality: z2h ascii/digit/kana, remove spaces/underscores, unify parens/dashes."""
    if s is None:
        return ""
    s = str(s).translate(JA_TRANS)
    s = jaconv.z2h(s, ascii=True, digit=True, kana=True)
    s = re.sub(r"[\s_ï¼¿]+", "", s).strip()
    return s

def anken_key(s: str) -> str:
    return ja_norm_strict(s)

# â˜… builderï¼¿ç‰©ä»¶å(10amè¨‚æ­£1027) [PDF]
# â˜… builderï¼¿ç‰©ä»¶å(10amè¨‚æ­£1027)ï¼¿å¤©äº•å‰²ã‚Šä»˜ã‘1éšï¼Š [Excel]
FILE_RE = re.compile(
    r"""^[â˜…ï¼Š]?\s*
        (?P<builder>.+?)[ï¼¿_]\s*         # allow ï¼¿ or _
        (?P<bukken>.+?)                  # ç‰©ä»¶å
        (?:                              # â–¼ time / è¨‚æ­£ block is OPTIONAL
            \(
                (?P<ts>[^)]*)
            \)\)*                        # tolerate stray ')'
        )?
        (?:[ï¼¿_]\s*å¤©äº•å‰²ã‚Šä»˜ã‘(?P<floor>å¹³å±‹|\d+éš)[ï¼Š*]?)?  # accept ï¼Š or *
        \s*$""",
    re.X,
)

def extract_tokens_from_name(name: str) -> dict:
    """
    Extract builder / ç‰©ä»¶å / floor info from BOM filename.

    Expected patterns:
      â˜…ãƒ“ãƒ«ãƒ€ãƒ¼ï¼¿ç‰©ä»¶å(â€¦)(â€¦)ï¼¿ å¤©äº•å‰²ã‚Šä»˜ã‘2éšï¼Š.xls
      â˜…ãƒ“ãƒ«ãƒ€ãƒ¼ï¼¿ç‰©ä»¶å(â€¦)(â€¦).pdf
      â˜…ãƒ“ãƒ«ãƒ€ãƒ¼ï¼¿ç‰©ä»¶åï¼¿ å¤©äº•å‰²ã‚Šä»˜ã‘å¹³å±‹ï¼Š.xls
      â˜…ãƒ“ãƒ«ãƒ€ãƒ¼ï¼¿ç‰©ä»¶å.pdf

    Strategy:
      1) Manual split:
         - strip extension
         - strip leading â˜…/ï¼Š
         - split at first ï¼¿ / _
         - cut off å¤©äº•å‰²ã‚Šä»˜ã‘ block
         - bukken keeps (3å·æ£Ÿ) etc, but drops (4pm)/(è¨‚æ­£1030)
      2) If that fails, fallback to FILE_RE.
    """
    if not name:
        return {}

    # ---- strip extension, normalise parens ----
    base, _ = os.path.splitext(name)
    nm = base.replace("ï¼ˆ", "(").replace("ï¼‰", ")")

    # remove leading marks
    nm2 = nm.lstrip("â˜…ï¼Š ").strip()

    builder = None
    bukken  = None
    floor   = None

    # ---- 1) manual parse ----
    parts = re.split(r"[ï¼¿_]", nm2, maxsplit=1)
    if len(parts) >= 2:
        builder_candidate = parts[0].strip()
        rest              = parts[1].strip()

        # split off å¤©äº•å‰²ã‚Šä»˜ã‘ part
        rest_main = rest
        floor_split = re.split(r"[ï¼¿_]\s*å¤©äº•å‰²ã‚Šä»˜ã‘", rest, maxsplit=1)
        if len(floor_split) == 2:
            rest_main, floor_part = floor_split[0].strip(), floor_split[1]
            if "å¹³å±‹" in floor_part:
                floor = "å¹³å±‹"
            else:
                mfl = re.search(r"(\d+)éš", floor_part)
                if mfl:
                    floor = f"{mfl.group(1)}éš"

        # ---- bukken: keep (3å·æ£Ÿ) etc, drop only è¨‚æ­£/æ™‚é–“ç³» ----
        def looks_like_suffix_paren(content: str) -> bool:
            c = content.strip()
            if not c:
                return False
            # è¨‚æ­£ç³»
            if "è¨‚æ­£" in c:
                return True
            # am/pm, 4pm, 10am, etc.
            if re.search(r"(am|pm)", c.lower()):
                return True
            # pure 3â€“4 digit token like 1030, 1600 (æ™‚åˆ»ã£ã½ã„)
            if re.fullmatch(r"\d{3,4}", c):
                return True
            return False

        bukken_candidate = rest_main.strip()
        for m in re.finditer(r"\([^)]*\)", rest_main):
            inner = m.group(0)[1:-1]
            if looks_like_suffix_paren(inner):
                # everything BEFORE this paren is ç‰©ä»¶å
                bukken_candidate = rest_main[:m.start()].strip()
                break

        if bukken_candidate:
            builder = builder_candidate
            bukken  = bukken_candidate

    # ---- 2) fallback to regex if bukken not resolved ----
    if not bukken:
        m = FILE_RE.match(nm2)
        if not m:
            return {}
        gd = m.groupdict()
        if not builder:
            builder = (gd.get("builder") or "").strip()
        bukken = (gd.get("bukken") or "").strip()
        floor  = gd.get("floor") or floor

    if not bukken:
        return {}

    return {
        "builder":      builder,
        "bukken":       bukken,
        "bukken_norm":  anken_key(bukken),
        "floor":        floor,
        "is_excel":     ("å¤©äº•å‰²ã‚Šä»˜ã‘" in nm),
    }

# ----------------- Version / Setup -----------------
file_path_token = os.path.join(os.getcwd(), "Access_token", "Access_token.txt")
logging.info(f"Access token path: {file_path_token}")
try:
    with open(file_path_token, "r", encoding="utf-8") as f:
        ACCESS_TOKEN_FILE = f.read().strip()
except Exception:
    ACCESS_TOKEN_FILE = ""
logging.info("Access token loaded.")

REPO_OWNER = "Nasiwak"
REPO_NAME  = "koujou_furiwake_bot"
CURRENT_VERSION = "1.4.6-BOM-HAISHA-UPLOAD"
try:
    Bot_Update(REPO_OWNER, REPO_NAME, CURRENT_VERSION, ACCESS_TOKEN_FILE)
except Exception as e:
    logging.warning(f"Update check failed (non-blocking): {e}")

# Fresh çµæœ book per run â€” save under ./Results
run_ts      = datetime.now().strftime("%Y%m%d_%H%M%S")
RESULTS_DIR = os.path.join(os.getcwd(), "Results")
os.makedirs(RESULTS_DIR, exist_ok=True)
excelsheet = os.path.join(RESULTS_DIR, f"çµæœ_{run_ts}.xlsx")

# ----------------- General helpers -----------------

def normalize_excel_id(val) -> str:
    """
    Convert Excel numeric IDs safely:
    1.0 -> '1'
    5.0 -> '5'
    'A' -> 'A'
    '01' -> '01'
    """
    if val is None:
        return ""

    # float like 1.0, 5.0
    if isinstance(val, float):
        if val.is_integer():
            return str(int(val))
        return str(val)

    # int
    if isinstance(val, int):
        return str(val)

    # string
    s = str(val).strip()
    if s.endswith(".0") and s.replace(".", "", 1).isdigit():
        return s[:-2]

    return s

def _norm_j(text: str) -> str:
    return (
        jaconv.h2z(text or "", ascii=True, digit=True, kana=True)
        .replace(" ", "").replace("ã€€", "")
        .replace("_", "").replace("ï¼¿", "")
    )

def jnorm(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    s = s.replace("_", "").replace("ï¼¿", "")
    s = re.sub(r"[ \t\u3000]+", " ", s).strip()
    return s

def _clean_header(h: str) -> str:
    s = jaconv.z2h(str(h or ""), ascii=True, digit=True, kana=False)
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[ \u3000\t]", "", s)
    return s

TIME_TOKENS = re.compile(
    r'(?:(?P<h>\d{1,2}):(?P<m>\d{2}))'
    r'|(?:(?P<h2>\d{1,2})(?P<m2>\d{2}))'
    r'|(?:(?P<h3>\d{1,2})(?P<ap>am|pm))',
    re.IGNORECASE
)

def extract_time_token(name: str):
    s = (name or "").lower()
    m = TIME_TOKENS.search(s)
    if not m:
        return None
    if m.group('h') and m.group('m'):
        hh = int(m.group('h')); mm = int(m.group('m'))
    elif m.group('h2') and m.group('m2'):
        hh = int(m.group('h2')); mm = int(m.group('m2'))
    else:
        hh = int(m.group('h3')); mm = 0
        if m.group('ap') == 'pm' and hh != 12:
            hh += 12
        if m.group('ap') == 'am' and hh == 12:
            hh = 0
    return f"{hh:02d}{mm:02d}"

def safe_set(ws, cell_ref, value):
    if isinstance(value, str) and value[:1] in ('=', '+'):
        ws[cell_ref] = "'" + value
    else:
        ws[cell_ref] = value

def _parse_floor(val) -> int:
    """
    Robust floor count from 'éš':
      - 'å¹³å±‹' -> 1
      - '2F', 'ï¼’éš' -> 2
      - '1,2,3F' / '1ãƒ»2ãƒ»3F' / '1/2/3F' / '1,2,3 éš' -> 3
      - numeric cell like 3 -> 3
    """
    if val is None or (isinstance(val, float) and math.isnan(val)):
        return 1
    s = str(val).strip()
    s = jaconv.z2h(s, ascii=True, digit=True, kana=False)
    if "å¹³å±‹" in s:
        return 1

    if re.fullmatch(r"\d+", s):
        n = int(s)
        return n if n in (1, 2, 3) else 1

    tokens = re.split(r"[,\u3001/ãƒ»\s]+", s)
    floors = set()
    for t in tokens:
        if not t:
            continue
        m = re.search(r"(\d+)", t)
        if m:
            n = int(m.group(1))
            if n in (1, 2, 3):
                floors.add(n)
    if floors:
        return len(floors)

    m = re.findall(r"(\d+)\s*(?:F|éš)", s, flags=re.IGNORECASE)
    if m:
        floors = {int(x) for x in m if int(x) in (1, 2, 3)}
        if floors:
            return len(floors)

    return 1

OK_FILL = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
NG_FILL = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

def color_result_row(ws, row_idx: int, result_text: str):
    headers = [ws.cell(row=1, column=c).value for c in range(1, ws.max_column + 1)]
    try:
        col_result = headers.index("çµæœ") + 1
    except ValueError:
        return
    cell = ws.cell(row=row_idx, column=col_result)
    if isinstance(result_text, str) and result_text.strip().upper().startswith("OK"):
        cell.fill = OK_FILL
    elif isinstance(result_text, str) and result_text.strip().upper().startswith("NG"):
        cell.fill = NG_FILL

def create_new_result_excel(path: str):
    wb = Workbook()
    ws = wb.active
    ws.title = "çµæœ"
    headers = [
        "æ¡ˆä»¶ç•ªå·", "ãƒ“ãƒ«ãƒ€ãƒ¼å", "ç‰©ä»¶å", "éš",
        "PDFæ•°", "Excelæ•°", "ä¸€è‡´PDF", "ä¸€è‡´Excel",
        "åˆ¤å®šç†ç”±", "çµæœ", "è³‡æ–™UP",
        "å·è»Š", "è¿½åŠ ä¸è¶³", "é…é€æ™‚ç‰¹è¨˜äº‹é …",
    ]
    for i, h in enumerate(headers, start=1):
        safe_set(ws, f"{get_column_letter(i)}1", h)
    style_headers(ws, 1, len(headers))
    ws.freeze_panes = "A2"
    ws.auto_filter.ref = f"A1:{get_column_letter(len(headers))}1"
    wb.save(path)
    logging.info(f"Created result workbook: {path}")


# ----------------- Styling -----------------
thin       = Side(style="thin", color="000000")
border_all = Border(left=thin, right=thin, top=thin, bottom=thin)
header_fill = PatternFill(start_color="EEEEEE", end_color="EEEEEE", fill_type="solid")
header_font = Font(bold=True)

def style_headers(ws, header_row: int, num_cols: int):
    # Header row height a bit taller
    ws.row_dimensions[header_row].height = 32

    for col in range(1, num_cols + 1):
        cell = ws.cell(row=header_row, column=col)
        cell.font      = header_font
        cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
        cell.fill      = header_fill
        cell.border    = border_all

def style_table(ws, first_data_row: int, last_row: int, last_col: int):
    if last_row < first_data_row:
        return

    headers = [ws.cell(row=1, column=c).value for c in range(1, ws.max_column + 1)]

    # Columns that should be centered (by header name)
    center_cols = set()
    for name in ("éš", "PDFæ•°", "Excelæ•°", "çµæœ", "è³‡æ–™UP", "æ¡ˆä»¶ç•ªå·", "å·è»Š"):
        if name in headers:
            center_cols.add(headers.index(name) + 1)

    # Columns where we expect long text and want wrap + left align
    wrap_cols = set()
    for name in ("ç‰©ä»¶å", "ä¸€è‡´Excel", "ä¸€è‡´PDF", "åˆ¤å®šç†ç”±", "é…é€æ™‚ç‰¹è¨˜äº‹é …", "è¿½åŠ ä¸è¶³"):
        if name in headers:
            wrap_cols.add(headers.index(name) + 1)

    # ----- Borders, alignment, row height -----
    for r in range(first_data_row, last_row + 1):
        # Slightly taller data rows for readability
        ws.row_dimensions[r].height = 27
        for c in range(1, last_col + 1):
            cell = ws.cell(row=r, column=c)
            cell.border = border_all

            if c in center_cols:
                cell.alignment = Alignment(
                    horizontal="center",
                    vertical="center",
                    wrap_text=True,
                )
            elif c in wrap_cols:
                cell.alignment = Alignment(
                    horizontal="left",
                    vertical="center",
                    wrap_text=True,
                )
            else:
                cell.alignment = Alignment(
                    horizontal="left",
                    vertical="center",
                    wrap_text=False,
                )

    # ----- Auto column width (respecting content length) -----
    max_width = {c: 0 for c in range(1, last_col + 1)}
    for r in range(1, last_row + 1):
        for c in range(1, last_col + 1):
            v  = ws.cell(row=r, column=c).value
            ln = len(str(v)) if v is not None else 0
            if ln > max_width[c]:
                max_width[c] = ln

    for c in range(1, last_col + 1):
        width = min(max(10, int(max_width[c] * 1.2)), 60)
        ws.column_dimensions[get_column_letter(c)].width = width


def add_ng_sheet_and_metrics(wb_path: str, counters: dict):
    wb = load_workbook(wb_path)
    ws = wb["çµæœ"]

    headers = [ws.cell(row=1, column=c).value for c in range(1, ws.max_column + 1)]
    try:
        idx_result = headers.index("çµæœ") + 1
    except ValueError:
        idx_result = ws.max_column

    ng = wb.create_sheet("NGä¸€è¦§")
    for i, h in enumerate(headers, 1):
        safe_set(ng, f"{get_column_letter(i)}1", h)
    style_headers(ng, 1, len(headers))
    ng.freeze_panes = "A2"
    ng.auto_filter.ref = f"A1:{get_column_letter(len(headers))}1"

    write_row = 2
    for r in range(2, ws.max_row + 1):
        res = str(ws.cell(row=r, column=idx_result).value or "").strip()
        if res.upper().startswith("NG"):
            for c in range(1, ws.max_column + 1):
                val = ws.cell(row=r, column=c).value
                safe_set(ng, f"{get_column_letter(c)}{write_row}", val)
            write_row += 1

    if write_row > 2:
        style_table(ng, 2, write_row - 1, len(headers))

    last = ws.max_row + 2
    safe_set(ws, f"A{last}",     "Summary")
    safe_set(ws, f"A{last+1}",   "åˆè¨ˆä»¶æ•°");        safe_set(ws, f"B{last+1}", counters.get("total_rows", 0))
    safe_set(ws, f"A{last+2}",   "OKä»¶æ•°");          safe_set(ws, f"B{last+2}", counters.get("ok", 0))
    safe_set(ws, f"A{last+3}",   "NGä»¶æ•°");          safe_set(ws, f"B{last+3}", counters.get("ng", 0))
    safe_set(ws, f"A{last+4}",   "é‡è¤‡PDFä»¶æ•°");      safe_set(ws, f"B{last+4}", counters.get("dup", 0))
    safe_set(ws, f"A{last+5}",   "æ™‚åˆ»ä¸ä¸€è‡´ä»¶æ•°");    safe_set(ws, f"B{last+5}", counters.get("ts_mismatch", 0))
    safe_set(ws, f"A{last+6}",   "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°");    safe_set(ws, f"B{last+6}", counters.get("uploaded", 0))

    style_table(ws, 2, ws.max_row, ws.max_column)
    wb.save(wb_path); wb.close()

# ----------------- SharePoint upload helpers -----------------
def _encode_share_url(url: str) -> str:
    """Encode a SharePoint URL into Graph /shares ID."""
    b = url.encode("utf-8")
    return "u!" + base64.urlsafe_b64encode(b).decode("ascii").rstrip("=")

def upload_usb_to_osaka_date(jp_date: str, usb_folder: Path) -> int:
    """
    Uploads the local â–½USB folder contents into:
      https://nskkogyo.sharepoint.com/sites/yanase/
        Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{jp_date}/â–½USB/{å·è»Š}/...

    Returns number of successfully uploaded files.
    """
    usb_folder = Path(usb_folder)
    if not usb_folder.exists():
        logging.info(f"USB folder does not exist; skipping upload: {usb_folder}")
        return 0

    target_url = (
        "https://nskkogyo.sharepoint.com/sites/yanase/"
        "Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
    ).format(date=jp_date)

    logging.info(f"[Graph] USB upload target URL (date folder): {target_url}")

    share_id = _encode_share_url(target_url)
    token    = get_access_token()
    headers  = {"Authorization": f"Bearer {token}"}

    # Resolve the *date* folder driveItem
    resp = requests.get(
        f"{GRAPH_BASE_URL}/shares/{share_id}/driveItem",
        headers=headers,
    )
    if not resp.ok:
        logging.error(
            f"[Graph] Failed to resolve date folder: {resp.status_code} {resp.text}"
        )
        return 0

    info      = resp.json()
    drive_id  = info["parentReference"]["driveId"]
    parent_id = info["id"]

    usb_name = usb_folder.name  # should be "â–½USB"
    uploaded = 0

    for root, _, files in os.walk(usb_folder):
        rel_root = Path(root).relative_to(usb_folder)

        for fname in files:
            local_path = Path(root) / fname

            # Build remote path so everything is under â–½USB on SharePoint:
            #   â–½USB/{truck}/... or â–½USB/file at root
            if rel_root == Path("."):
                remote_rel = str(PurePosixPath(usb_name) / fname)
            else:
                remote_rel = str(
                    PurePosixPath(usb_name)
                    / PurePosixPath(rel_root.as_posix())
                    / fname
                )

            put_url = (
                f"{GRAPH_BASE_URL}/drives/{drive_id}/items/{parent_id}:/"
                f"{remote_rel}:/content"
            )

            try:
                with open(local_path, "rb") as f:
                    put_resp = requests.put(put_url, headers=headers, data=f)
                if put_resp.status_code in (200, 201):
                    uploaded += 1
                    logging.info(f"[Graph] Uploaded {remote_rel} â†’ {target_url}")
                else:
                    logging.error(
                        f"[Graph] Upload failed for {remote_rel}: "
                        f"{put_resp.status_code} {put_resp.text}"
                    )
            except Exception as e:
                logging.error(f"[Graph] Exception during upload for {remote_rel}: {e}")

    logging.info(f"[Graph] USB upload completed. Files uploaded: {uploaded}")
    return uploaded

# ----------------- Factory key resolver -----------------
def resolve_factory_key(factory_label: str) -> str | None:
    s = factory_label.strip()
    if "å¤§é˜ª" in s:
        return "å¤§é˜ª"
    if "ä¹å·" in s:
        return "ä¹å·"
    if "æ ƒæœ¨" in s or "çœŸå²¡" in s:
        return "æ ƒæœ¨"
    if "åƒè‘‰" in s:
        return "åƒè‘‰"
    if "è±Šæ©‹" in s:
        return "è±Šæ©‹"
    if "æ»‹è³€" in s:
        return "æ»‹è³€"
    return None

# ----------------- Core (furiwake) -----------------
class process:
    def __init__(self, from_date, to_date) -> None:
        self.from_date = from_date        # å·¥å ´ (e.g. "å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿")
        self.to_date   = to_date          # "11æœˆ15æ—¥" etc.
        logging.info(f"Run params â€” å·¥å ´:{self.from_date} / æ—¥ä»˜:{self.to_date}")

        self.base_dir      = Path.cwd()
        self.usb_folder    = r'â–½USB'
        self.haisha_folder = r'é…è»Šè¡¨'
        self.bom_root: Path | None = None

        # Fresh start: wipe BOM, USB, é…è»Šè¡¨ and recreate for this run
        self.reset_run_folders()

        # çµæœ book
        self.excelsheet = excelsheet
        create_new_result_excel(self.excelsheet)

        # ====== BOM + é…è»Š download via bom_downloader.py ======
        factory_key = resolve_factory_key(self.from_date)
        if factory_key:
            try:
                self.bom_root = download_factory_bom_for_date(
                    factory_key,
                    self.to_date,
                    self.base_dir,
                )
                if self.bom_root is None:
                    logging.warning(
                        f"No BOM files downloaded for factory '{factory_key}' "
                        f"and date '{self.to_date}'."
                    )
                else:
                    logging.info(f"BOM root for this run: {self.bom_root}")
            except Exception as e:
                logging.error(f"BOM download failed for factory '{factory_key}': {e}")
                self.bom_root = None

            # é…è»Šè¡¨: only for å¤§é˜ª
            if factory_key == "å¤§é˜ª":
                try:
                    haisha_root = download_osaka_haisha_for_date(
                        self.to_date,
                        self.base_dir,
                    )
                    if haisha_root is None:
                        logging.warning(
                            f"é…è»Šè¡¨ not downloaded for å¤§é˜ª, date '{self.to_date}'. "
                            f"é…è»Šè¡¨ folder may be empty."
                        )
                    else:
                        logging.info(f"é…è»Šè¡¨ saved under: {haisha_root}")
                except Exception as e:
                    logging.error(
                        f"é…è»Šè¡¨ download failed for å¤§é˜ª, date '{self.to_date}': {e}"
                    )
            elif factory_key == "æ ƒæœ¨":
                try:
                    haisha_root = download_tochigi_haisha_for_date(
                        self.to_date,
                        self.base_dir,
                    )
                    if haisha_root is None:
                        logging.warning(
                            f"é…è»Šè¡¨ not downloaded for æ ƒæœ¨, date '{self.to_date}'. "
                            f"é…è»Šè¡¨ folder may be empty."
                        )
                    else:
                        logging.info(f"æ ƒæœ¨ é…è»Šè¡¨ saved under: {haisha_root}")
                except Exception as e:
                    logging.error(f"é…è»Šè¡¨ download failed for æ ƒæœ¨, date '{self.to_date}': {e}")


        else:
            logging.info(
                f"No BOM mapping for å·¥å ´={self.from_date}; skipping BOM/é…è»Š download."
            )

        # Metrics
        self.metrics = dict(
            total_rows=0, ok=0, ng=0, dup=0, ts_mismatch=0, uploaded=0
        )

        self.missing_bom_rows = []

        # ğŸ”¹ MAIN FURIWAKE LOGIC ğŸ”¹
        # This reads é…è»Šè¡¨, matches against BOM, writes çµæœ sheet & moves files into â–½USB
        self.compare_ankenmei()
        self.write_missing_bom_sheet() 

        factory_key = resolve_factory_key(self.from_date)
        usb_path = self.base_dir / self.usb_folder

        if factory_key == "å¤§é˜ª":
            try:
                uploaded_count = upload_usb_to_osaka_date(self.to_date, usb_path)
            except Exception as e:
                logging.error(f"USB upload error (å¤§é˜ª): {e}")
                uploaded_count = 0

        elif factory_key == "æ ƒæœ¨":
            try:
                uploaded_count = upload_usb_to_tochigi_date(self.to_date, usb_path)
            except Exception as e:
                logging.error(f"USB upload error (æ ƒæœ¨): {e}")
                uploaded_count = 0

        else:
            uploaded_count = 0

        # è³‡æ–™UP column:
        # - If at least one file uploaded â†’ è³‡æ–™UP = OK for çµæœ==OK rows, NG otherwise
        # - If nothing uploaded â†’ all è³‡æ–™UP = NG
        upload_status = "OK" if uploaded_count > 0 else "NG"
        self.shiryou_upload_status(upload_status)

        # Add NGä¸€è¦§ sheet + summary block at the end
        add_ng_sheet_and_metrics(self.excelsheet, self.metrics)

    def reset_run_folders(self):
        """Remove and recreate USB, é…è»Šè¡¨, and BOM folders fully every run."""
        base = Path(self.base_dir)

        # --- 1) USB ---
        usb = base / "â–½USB"
        if usb.exists():
            shutil.rmtree(usb, ignore_errors=True)
            logging.info("Removed folder: â–½USB")
        usb.mkdir(parents=True, exist_ok=True)
        logging.info("Created folder: â–½USB")

        # --- 2) é…è»Šè¡¨ ---
        haisha = base / "é…è»Šè¡¨"
        if haisha.exists():
            shutil.rmtree(haisha, ignore_errors=True)
            logging.info("Removed folder: é…è»Šè¡¨")
        haisha.mkdir(parents=True, exist_ok=True)
        logging.info("Created folder: é…è»Šè¡¨")

        # --- 3) BOM ---
        bom = base / "BOM"
        if bom.exists():
            shutil.rmtree(bom, ignore_errors=True)
            logging.info("Removed folder: BOM (full)")
        bom.mkdir(parents=True, exist_ok=True)
        logging.info("Created folder: BOM (empty)")

        # Today's BOM folder
        today_dir = bom / self.to_date
        today_dir.mkdir(parents=True, exist_ok=True)
        self.bom_root = str(today_dir)
        logging.info(f"Prepared BOM folder for this date: {today_dir}")

    def create_clear_folder(self, path):
        # Kept for backward compatibility (not used now)
        if os.path.exists(path):
            shutil.rmtree(path)
            logging.info(f"Removed folder: {path}")
        os.makedirs(path, exist_ok=True)
        logging.info(f"Created folder: {path}")

    def count_uploaded_files(self) -> int:
        """Count how many files exist under â–½USB (for 'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°' metric)."""
        base = Path(self.usb_folder)
        if not base.exists():
            return 0
        total = 0
        for _root, _dirs, files in os.walk(base):
            total += len(files)
        return total


    # ---- BOM artifact lookup ----
    def find_artifacts_for_anken(self, anken_key_norm: str):
        """
        Look for BOM files under ./BOM/<æ—¥ä»˜>/ (downloaded by bom_downloader).
        Fallback: ./BOM if date-specific folder doesn't exist.

        1st pass: strict bukken_norm == anken_key_norm
        2nd pass: (if strict empty) loose match where one name contains the other.
        """
        # ---- decide base BOM folder ----
        if self.bom_root is not None:
            base = Path(self.bom_root)
        else:
            candidate = self.base_dir / "BOM" / self.to_date
            base = candidate if candidate.exists() else (self.base_dir / "BOM")

        if not base.exists():
            logging.info(f"No BOM folder found at: {base}")
            return {"pdfs": [], "excels": [], "floors": set()}

        def detect_floor(name: str) -> str:
            FLOOR_PATTERNS = {
                "1F": [r"\b1F\b", r"ï¼‘ï¼¦", r"1éš", r"ï¼‘éš", r"ä¸€éš"],
                "2F": [r"\b2F\b", r"ï¼’ï¼¦", r"2éš", r"ï¼’éš", r"äºŒéš"],
                "3F": [r"\b3F\b", r"ï¼“ï¼¦", r"3éš", r"ï¼“éš", r"ä¸‰éš"],
                "å¹³å±‹": [r"å¹³å±‹"],
            }
            s = jnorm(name)
            for fl, pats in FLOOR_PATTERNS.items():
                for p in pats:
                    if re.search(p, s, flags=re.IGNORECASE):
                        return fl
            return "UNKNOWN"

        strict_pdfs, strict_excels, strict_floors = [], [], set()
        loose_pdfs,  loose_excels,  loose_floors  = [], [], set()

        for root, _, files in os.walk(base):
            for file in files:
                low = file.lower()
                if not (
                    low.endswith(".pdf")
                    or low.endswith((".xlsx", ".xls", ".csv"))
                ):
                    continue

                toks = extract_tokens_from_name(file)
                if not toks:
                    continue

                bukken_norm = toks.get("bukken_norm", "")
                if not bukken_norm:
                    continue

                full_path = os.path.join(root, file)

                # ---- strict match ----
                if bukken_norm == anken_key_norm:
                    if low.endswith(".pdf"):
                        strict_pdfs.append(full_path)
                    else:
                        strict_excels.append(full_path)
                        strict_floors.add(detect_floor(file))
                    continue

                # ---- loose match: one contains the other ----
                if anken_key_norm and (
                    anken_key_norm in bukken_norm or bukken_norm in anken_key_norm
                ):
                    if low.endswith(".pdf"):
                        loose_pdfs.append(full_path)
                    else:
                        loose_excels.append(full_path)
                        loose_floors.add(detect_floor(file))

        # Prefer strict, fallback to loose
        if strict_pdfs or strict_excels:
            return {
                "pdfs":   strict_pdfs,
                "excels": strict_excels,
                "floors": strict_floors,
            }
        else:
            return {
                "pdfs":   loose_pdfs,
                "excels": loose_excels,
                "floors": loose_floors,
            }

    def create_folder_and_move_matched_files(self, matched_files):
        try:
            go_folder  = os.path.join(self.usb_folder, self.å·è»Š)
            waritsuke  = os.path.join(go_folder, "å‰²ä»˜å›³")
            os.makedirs(waritsuke, exist_ok=True)
            for file_path in matched_files:
                if not os.path.isfile(file_path):
                    continue
                name = os.path.basename(file_path)
                if name.lower().endswith(".pdf"):
                    dest = os.path.join(waritsuke, name)
                elif name.lower().endswith((".xlsx", ".xls", ".csv")):
                    dest = os.path.join(go_folder, name)
                else:
                    continue
                shutil.move(file_path, dest)
                logging.info(f"MOVED: {name} -> {dest}")
            return "OK"
        except Exception as e:
            logging.error(f"Move error: {e}")
            return "NG"

    def shiryou_upload_status(self, status):
        fp = self.excelsheet
        wb = load_workbook(fp)
        ws = wb["çµæœ"]
        headers = [ws.cell(row=1, column=c).value for c in range(1, ws.max_column + 1)]
        if "è³‡æ–™UP" not in headers:
            wb.close()
            logging.info("è³‡æ–™UP column not present; skip updating.")
            return
        col_kekka   = headers.index("çµæœ") + 1
        col_shiryou = headers.index("è³‡æ–™UP") + 1

        row_count = 0
        for row in ws.iter_rows(min_row=2):
            v = row[col_kekka - 1].value
            if v is not None and str(v) != "":
                row_count += 1
            else:
                break

        for i, row in enumerate(ws.iter_rows(min_row=2), start=1):
            if i > row_count:
                break
            r    = str(row[col_kekka - 1].value or "")
            cell = row[col_shiryou - 1]
            val  = "OK" if (status == "OK" and r == "OK") else "NG"
            if isinstance(val, str) and val.startswith(('=', '+')):
                cell.value = "'" + val
            else:
                cell.value = val

        wb.save(fp); wb.close()
        logging.info("è³‡æ–™UP updated.")
    
    def write_missing_bom_sheet(self):
        """Create sheet 'æœªUPæ¡ˆä»¶' listing é…é€å…ˆ with zero BOM files."""
        if not self.missing_bom_rows:
            logging.info("No missing BOM rows â†’ æœªUPæ¡ˆä»¶ sheet not created.")
            return

        wb = load_workbook(self.excelsheet)
        ws = wb.create_sheet("æœªUPæ¡ˆä»¶")

        headers = ["å·è»Š", "æ¡ˆä»¶ç•ªå·", "Builderå", "é…é€å…ˆ", "å¿…è¦éš"]
        for col, h in enumerate(headers, start=1):
            ws.cell(row=1, column=col).value = h

        row = 2
        for item in self.missing_bom_rows:
            ws.cell(row=row, column=1).value = item["go"]
            ws.cell(row=row, column=2).value = item["anken"]
            ws.cell(row=row, column=3).value = item["builder"]
            ws.cell(row=row, column=4).value = item["bukken"]
            ws.cell(row=row, column=5).value = item["floors"]
            row += 1

        style_headers(ws, 1, len(headers))
        if row > 2:
            style_table(ws, 2, row - 1, len(headers))

        wb.save(self.excelsheet)
        wb.close()
        logging.info("æœªUPæ¡ˆä»¶ sheet created.")


    # ------------ é…è»Šèª­ã¿è¾¼ã¿ & åˆ¤å®š ------------"

    def compare_ankenmei(self):
        folder_path = os.path.join(os.getcwd(), "é…è»Šè¡¨")
        xls_path    = None
        xlsx_path   = None

        # ---- Find é…è»Š file ----
        for root, _, files in os.walk(folder_path):
            for f in files:
                if "é…è»Š" in f and f.lower().endswith(".xls"):
                    xls_path = os.path.join(root, f); break
                if "é…è»Š" in f and f.lower().endswith(".xlsx"):
                    xlsx_path = os.path.join(root, f); break
            if xls_path or xlsx_path:
                break

        if not xls_path and not xlsx_path:
            logging.error("é…è»Šãƒ•ã‚¡ã‚¤ãƒ«(.xls/.xlsx)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return

        # ---- If .xls â†’ convert ----
        temp_xlsx = os.path.join(folder_path, "temp_unlocked.xlsx")
        if xlsx_path:
            temp_xlsx = xlsx_path
        else:
            try:
                import win32com.client
                password = "nsk"
                excel    = win32com.client.Dispatch("Excel.Application")
                excel.DisplayAlerts = False
                wb_x = excel.Workbooks.Open(xls_path, False, True, None, password)
                wb_x.SaveAs(temp_xlsx, FileFormat=51)
                wb_x.Close(False)
                excel.Application.Quit()
            except Exception as e:
                logging.error(f".xls ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return

        # ---- Read é…è»Š ----
        try:
            raw = pd.read_excel(temp_xlsx, sheet_name="äºˆå®š")
        except Exception as e:
            logging.error(f"é…è»Šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­è¾¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return

        df = raw.copy()
        norm_map = {_clean_header(c): c for c in df.columns}

        def pick(name):
            return norm_map.get(name)

        col_bukken   = pick("é…é€å…ˆ")
        col_kai      = pick("éš")
        col_builder  = pick("ãƒ“ãƒ«ãƒ€ãƒ¼å") or pick("å¾—æ„å…ˆå")
        col_anken    = pick("æ¡ˆä»¶ç•ªå·")
        col_go       = pick("å·è»Š") or pick("å·")
        col_status   = pick("è¿½åŠ ä¸è¶³")
        col_tokki    = pick("é…é€æ™‚ç‰¹è¨˜äº‹é …")

        if not (col_bukken and col_kai):
            logging.error("äºˆå®šã‚·ãƒ¼ãƒˆã«å¿…è¦åˆ—ä¸è¶³: é…é€å…ˆ/éš ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        df = df.replace({pd.NA: None})
        df = df[~(df[col_bukken].isna())].copy()

        wb = load_workbook(self.excelsheet)
        ws = wb["çµæœ"]
        write_row = 2
        total_ok = total_ng = 0

        # =============================
        # ğŸ”¥ MAIN é…è»Š â†’ BOM MATCH LOOP
        # =============================
        for _, r in df.iterrows():
            anken_no = normalize_excel_id(r.get(col_anken, "")) if col_anken else ""
            builder  = str(r.get(col_builder, "") or "").strip() if col_builder else ""
            bukken   = str(r.get(col_bukken, "") or "").strip()
            floors   = _parse_floor(r.get(col_kai, ""))

            # å·è»Š
            go_raw = r.get(col_go, "") if col_go else ""
            go = normalize_excel_id(go_raw).upper()


            # è¿½åŠ ä¸è¶³
            status = str(r.get(col_status, "") or "").strip() if col_status else ""

            # ç‰¹è¨˜äº‹é …
            tokki = str(r.get(col_tokki, "") or "").strip() if col_tokki else ""

            # ---- Missing é…é€å…ˆ ----
            if not bukken:
                vals = [
                    anken_no, builder, "", floors,
                    0, 0, "", "",
                    "é…é€å…ˆ(ç‰©ä»¶å)æ¬ è½", "NG", "NG",
                    go, status, tokki,
                ]
                for i, v in enumerate(vals, start=1):
                    safe_set(ws, f"{get_column_letter(i)}{write_row}", v)
                color_result_row(ws, write_row, "NG")
                write_row += 1
                total_ng  += 1
                continue

            # ---- Find BOM artifacts ----
            key_norm = anken_key(bukken)
            arts     = self.find_artifacts_for_anken(key_norm)
            pdf_count   = len(arts["pdfs"])
            excel_count = len(arts["excels"])

            matched_pdf_name   = os.path.basename(arts["pdfs"][0]) if pdf_count > 0 else ""
            matched_excel_name = " | ".join(sorted(os.path.basename(p) for p in arts["excels"])) if excel_count > 0 else ""

            # --------------------------
            # ğŸ”¥ NEW FEATURE:
            # If BOTH PDF=0 AND Excel=0 â†’ add to æœªUPæ¡ˆä»¶ list
            # --------------------------
            if pdf_count == 0 and excel_count == 0:
                self.missing_bom_rows.append({
                    "go": go,
                    "anken": anken_no,
                    "builder": builder,
                    "bukken": bukken,
                    "floors": floors,
                })

            # ---- åˆ¤å®š ----
            reasons = []
            if pdf_count != 1:
                reasons.append(f"PDFæ•°={pdf_count} (æœŸå¾…:1)")
            if excel_count != floors:
                reasons.append(f"Excelæ•°={excel_count} (æœŸå¾…:{floors})")

            result  = "OK" if not reasons else "NG"
            shiryou = "NG"

            # ---- FURIWAKE (OK only) ----
            if result == "OK" and (arts["pdfs"] or arts["excels"]):
                try:
                    self.å·è»Š = go or "æœªæŒ‡å®š"
                    move_status = self.create_folder_and_move_matched_files(
                        arts["pdfs"] + arts["excels"]
                    )
                except Exception as e:
                    logging.error(f"Furiwake move error for '{bukken}': {e}")

            # ---- Write row ----
            vals = [
                anken_no, builder, bukken, floors,
                pdf_count, excel_count,
                matched_pdf_name, matched_excel_name,
                " / ".join(reasons), result, shiryou,
                go, status, tokki,
            ]

            for i, v in enumerate(vals, start=1):
                if isinstance(v, float) and math.isnan(v):
                    v = ""
                safe_set(ws, f"{get_column_letter(i)}{write_row}", v)

            color_result_row(ws, write_row, result)
            write_row += 1
            total_ok  += int(result == "OK")
            total_ng  += int(result == "NG")

        # ---- Style ----
        if write_row > 2:
            style_table(ws, 2, write_row - 1, ws.max_column)

        # ---- Update metrics ----
        self.metrics["total_rows"] = write_row - 2
        self.metrics["ok"]        = total_ok
        self.metrics["ng"]        = total_ng

        wb.save(self.excelsheet)
        wb.close()


# ----------------- GUI -----------------
class DateHandler:
    def __init__(self, from_date, to_date, app=None):
        self.from_date = from_date
        self.to_date   = to_date
        self.app       = app
        self.process_dates()

    def process_dates(self):
        if self.app:
            self.app.set_busy(True)
        try:
            result = process(self.from_date, self.to_date)
            logging.info(f"From date: {self.from_date}")
            logging.info(f"To date: {self.to_date}")
            return result
        finally:
            if self.app:
                self.app.set_busy(False)

class App:
    def __init__(self, 
        root: ctk.CTk,
        å·¥å ´: str,
        æ—¥ä»˜: str,
    ):
        self.root = root
        self.å·¥å ´ = å·¥å ´
        self.æ—¥ä»˜ = æ—¥ä»˜
        self.root.title("å·¥å ´æŒ¯ã‚Šåˆ†ã‘ï¼ˆå¤§é˜ªï¼‰")
        self.root.geometry("640x480")
        ctk.set_appearance_mode("Light")

        self.subtitle = ctk.CTkLabel(
            self.root,
            text="é…è»Šè¡¨ãƒ»BOMã‚’å–å¾— â†’ USBä»•åˆ†ã‘ â†’ çµæœå‡ºåŠ› â†’ USBã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            font=("Meiryo UI", 12),
        )
        self.subtitle.pack(pady=(10, 0))

        frm = ctk.CTkFrame(self.root); frm.pack(fill="x", padx=16, pady=12)

        row1 = ctk.CTkFrame(frm); row1.pack(fill="x", pady=6)
        ctk.CTkLabel(row1, text="å·¥å ´", width=80, anchor="e").pack(side="left", padx=6)
        self.from_date_entry = ctk.CTkOptionMenu(
            row1,
            values=[self.å·¥å ´],
        )
        self.from_date_entry.pack(side="left", padx=6)

        row2 = ctk.CTkFrame(frm); row2.pack(fill="x", pady=6)
        ctk.CTkLabel(row2, text="æ—¥ä»˜", width=80, anchor="e").pack(side="left", padx=6)
        self.to_date_entry = ctk.CTkEntry(row2, placeholder_text="ä¾‹) 11æœˆ15æ—¥")
        self.to_date_entry.pack(side="left", padx=6, fill="x", expand=True)
        self.to_date_entry.delete(0, tk.END)
        self.to_date_entry.insert(0, self.æ—¥ä»˜)

        self.progress = ctk.CTkProgressBar(self.root)
        self.progress.pack(fill="x", padx=16, pady=(6, 0))
        self.progress.set(0)
        self.status = ctk.CTkLabel(self.root, text="å¾…æ©Ÿä¸­", anchor="center")
        self.status.pack(pady=(4, 8))

        btns = ctk.CTkFrame(self.root); btns.pack(pady=8)
        self.start_button = ctk.CTkButton(btns, text="å®Ÿè¡Œ", width=140, command=self.on_start)
        self.start_button.pack(side="left", padx=10)
        self.close_button = ctk.CTkButton(btns, text="é–‰ã˜ã‚‹", width=120, command=self.root.destroy)
        self.close_button.pack(side="left", padx=10)

        self.footer = ctk.CTkLabel(
            self.root,
            text=f"Version {CURRENT_VERSION}   Â© {datetime.now().year} Nasiwak",
        )
        self.footer.pack(pady=(8, 12))

        self.root.after(15000,self.on_start)

    def set_busy(self, busy: bool):
        try:
            if busy:
                self.start_button.configure(state="disabled")
                self.progress.set(0.3)
                self.status.configure(text="å‡¦ç†ä¸­â€¦")
            else:
                self.start_button.configure(state="normal")
                self.progress.set(1.0)
                self.status.configure(text="å®Œäº†")
            self.root.update_idletasks()
        except Exception:
            pass

    def on_start(self):
        factory = self.from_date_entry.get()
        date_txt = (self.to_date_entry.get() or "").strip()
        if not date_txt:
            self.status.configure(text="æ—¥ä»˜ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        DateHandler(factory, date_txt, app=self)
        self.root.quit()
        self.root.destroy()

if __name__ == "__main__":
    args = parse_args()
    
    root = ctk.CTk()
    app = App(
        root,
        å·¥å ´ = args.å·¥å ´,
        æ—¥ä»˜ = args.æ—¥ä»˜,
    )
    root.mainloop()
