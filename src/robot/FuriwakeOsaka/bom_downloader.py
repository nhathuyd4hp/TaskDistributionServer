# -*- coding: utf-8 -*-
"""
bom_downloader.py â€” Download factory BOM files from SharePoint via Graph

Supports multiple factories via SharePoint /shares:

- Each factory has a base URL template with a `{date}` placeholder.
- The `{date}` part is typically the Japanese-style folder name like "11æœˆ15æ—¥"
  or "11æœˆ15æ—¥é…é€åˆ†", passed in from Main.py.

- All xls/xlsx/xlsm/pdf/csv files are downloaded into:
    <base_dir>/BOM/<date>/

- Designed to be called from Main.py:
    download_factory_bom_for_date("å¤§é˜ª", "11æœˆ15æ—¥", Path.cwd())

For backward compatibility, the old helper:
    download_osaka_bom_for_date("11æœˆ15æ—¥", Path.cwd())
is still provided and internally calls the generic function.
"""

import logging
import base64
from pathlib import Path

import requests
from token_manager import get_access_token
from config import BASE_URL  # same BASE_URL used in graph_downloader.py
import os
from pathlib import Path, PurePosixPath


GRAPH_BASE_URL = "https://graph.microsoft.com/v1.0"


# ---------------------------------------------------------------------------
# Factory â†’ SharePoint URL templates
# ---------------------------------------------------------------------------
# NOTE:
# - `{date}` will be replaced with jp_folder_name passed from Main.py.
# - Make sure jp_folder_name matches the actual folder naming, e.g.:
#     "11æœˆ15æ—¥"            â†’ .../11æœˆ15æ—¥
#     "11æœˆ15æ—¥é…é€åˆ†"      â†’ .../11æœˆ15æ—¥é…é€åˆ†
# ---------------------------------------------------------------------------
FACTORY_SHARE_URLS: dict[str, str] = {
    # å¤§é˜ª
    "å¤§é˜ª": (
        "https://nskkogyo.sharepoint.com/sites/yanase/"
        "Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/ğŸ”¹é–¢è¥¿å·¥å ´ç¢ºå®šãƒ‡ãƒ¼ã‚¿ğŸ”¹"
    ),

    # æ ƒæœ¨ï¼ˆçœŸå²¡å·¥å ´ï¼‰
    "æ ƒæœ¨": (
        "https://nskkogyo.sharepoint.com/sites/mouka/"
        "Shared Documents/çœŸå²¡å·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/æ ƒæœ¨å·¥å ´ç¢ºå®šãƒ‡ãƒ¼ã‚¿"
    ),

    # åƒè‘‰
    "åƒè‘‰": (
        "https://nskkogyo.sharepoint.com/sites/nskhome/"
        "Shared Documents/åƒè‘‰å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
    ),

    # è±Šæ©‹
    "è±Šæ©‹": (
        "https://nskkogyo.sharepoint.com/sites/toyohashi/"
        "Shared Documents/è±Šæ©‹å·¥å ´è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
    ),

    # ä¹å·
    "ä¹å·": (
        "https://nskkogyo.sharepoint.com/sites/kyuusyuukouzyou/"
        "Shared Documents/ä¹å·å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿ãƒ¼/{date}/è£½é€ "
    ),

    # æ»‹è³€
    "æ»‹è³€": (
        "https://nskkogyo.sharepoint.com/sites/shiga/"
        "Shared Documents/æ»‹è³€å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/è£½é€ ã€€æ‰‹é…æ¸ˆã¿(DLæ¸ˆã¿)"
    ),
}


def _encode_share_url(url: str) -> str:
    """
    Graph /shares API expects a base64url-encoded share URL with 'u!' prefix.
    """
    b64 = base64.b64encode(url.encode("utf-8")).decode("ascii")
    b64 = b64.rstrip("=")          # remove padding
    b64 = b64.replace("+", "-").replace("/", "_")
    return f"u!{b64}"


def download_factory_bom_for_date(
    factory_label: str,
    jp_folder_name: str,
    base_dir: Path,
) -> Path | None:
    """
    Generic factory BOM downloader.

    factory_label:
        Factory key as used in FACTORY_SHARE_URLS, e.g. "å¤§é˜ª", "æ ƒæœ¨", "åƒè‘‰", "è±Šæ©‹", "ä¹å·", "æ»‹è³€".

    jp_folder_name:
        e.g. "11æœˆ15æ—¥" or "11æœˆ15æ—¥é…é€åˆ†"
        (must match the actual folder naming on SharePoint).

    base_dir:
        Usually Path.cwd() from Main.py.

    Returns:
        Path to <base_dir>/BOM/<jp_folder_name> if files were downloaded,
        or None if folder not found / no files / factory not configured.
    """
    factory_key = factory_label.strip()

    if factory_key not in FACTORY_SHARE_URLS:
        logging.info(
            f"[Graph] No SharePoint BOM path configured for factory: {factory_key}"
        )
        return None

    url_template = FACTORY_SHARE_URLS[factory_key]
    target_url = url_template.format(date=jp_folder_name)

    logging.info(
        f"[Graph] BOM target SharePoint URL for factory '{factory_key}': {target_url}"
    )

    share_id = _encode_share_url(target_url)
    list_url = f"{BASE_URL}/shares/{share_id}/driveItem/children"
    headers = {"Authorization": f"Bearer {get_access_token()}"}

    try:
        resp = requests.get(list_url, headers=headers)
    except Exception as e:
        logging.error(
            f"[Graph] BOM list request failed for factory '{factory_key}': {e}"
        )
        return None

    if resp.status_code == 404:
        logging.warning(
            f"[Graph] BOM folder not found for factory '{factory_key}', "
            f"date '{jp_folder_name}'"
        )
        return None

    try:
        resp.raise_for_status()
    except Exception as e:
        logging.error(
            f"[Graph] BOM list error for factory '{factory_key}': {e} | "
            f"body={resp.text[:500]}"
        )
        return None

    items = resp.json().get("value", [])
    if not items:
        logging.warning(
            f"[Graph] BOM folder is empty for factory '{factory_key}', "
            f"date '{jp_folder_name}'"
        )
        return None

    dest_root = Path(base_dir) / "BOM" / jp_folder_name
    dest_root.mkdir(parents=True, exist_ok=True)

    count = 0
    for it in items:
        # skip sub-folders
        if "file" not in it:
            continue

        name = it.get("name", "")
        # only BOM-related formats
        if not any(
            name.lower().endswith(ext)
            for ext in (".xlsx", ".xlsm", ".xls", ".pdf", ".csv")
        ):
            continue

        drive_id = it["parentReference"]["driveId"]
        file_id = it["id"]
        dl_url = f"{BASE_URL}/drives/{drive_id}/items/{file_id}/content"

        logging.info(
            f"[Graph] Downloading BOM file for '{factory_key}': {name}"
        )
        try:
            r = requests.get(dl_url, headers=headers, stream=True)
            r.raise_for_status()
        except Exception as e:
            logging.error(
                f"[Graph] Download failed for {name} (factory '{factory_key}'): "
                f"{e} | body={getattr(r, 'text', '')[:200]}"
            )
            continue

        out_path = dest_root / name
        with open(out_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        count += 1

    logging.info(
        f"[Graph] BOM download finished for factory '{factory_key}' â€” "
        f"{count} files saved under: {dest_root}"
    )
    return dest_root if count > 0 else None

# ---------------------------------------------------------------------------
# é…è»Šè¡¨ (å¤§é˜ª) downloader â€” same Graph /shares style, different folder
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# é…è»Šè¡¨ (å¤§é˜ª) downloader â€” now writes into ./é…è»Šè¡¨
# ---------------------------------------------------------------------------

OSAKA_HAISHA_SHARE_URL_TEMPLATE = (
    "https://nskkogyo.sharepoint.com/sites/yanase/"
    "Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
)

def download_osaka_haisha_for_date(jp_folder_name: str, base_dir: Path) -> Path | None:
    """
    Download é…è»Šè¡¨ Excel(s) for å¤§é˜ª from:

        https://nskkogyo.sharepoint.com/sites/yanase/
        Shared Documents/å¤§é˜ªå·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}

    and save them under:

        <base_dir>/é…è»Šè¡¨/

    Only files whose name contains 'é…è»Š' and ends with .xls / .xlsx are downloaded.
    """
    target_url = OSAKA_HAISHA_SHARE_URL_TEMPLATE.format(date=jp_folder_name)

    logging.info(f"[Graph] é…è»Šè¡¨ target SharePoint URL (å¤§é˜ª): {target_url}")

    share_id = _encode_share_url(target_url)
    list_url = f"{BASE_URL}/shares/{share_id}/driveItem/children"
    headers = {"Authorization": f"Bearer {get_access_token()}"}

    try:
        resp = requests.get(list_url, headers=headers)
    except Exception as e:
        logging.error(f"[Graph] é…è»Šè¡¨ list request failed (å¤§é˜ª): {e}")
        return None

    if resp.status_code == 404:
        logging.warning(f"[Graph] é…è»Šè¡¨ folder not found for å¤§é˜ª, date '{jp_folder_name}'")
        return None

    try:
        resp.raise_for_status()
    except Exception as e:
        logging.error(f"[Graph] é…è»Šè¡¨ list error (å¤§é˜ª): {e} | body={resp.text[:500]}")
        return None

    items = resp.json().get("value", [])
    if not items:
        logging.warning(f"[Graph] é…è»Šè¡¨ folder is empty for å¤§é˜ª, date '{jp_folder_name}'")
        return None

    dest_root = Path(base_dir) / "é…è»Šè¡¨"
    dest_root.mkdir(parents=True, exist_ok=True)

    count = 0
    for it in items:
        if "file" not in it:
            continue

        name = it.get("name", "")
        lower = name.lower()

        if "é…è»Š" not in name:
            continue
        if not (lower.endswith(".xls") or lower.endswith(".xlsx")):
            continue

        drive_id = it["parentReference"]["driveId"]
        file_id  = it["id"]
        dl_url   = f"{BASE_URL}/drives/{drive_id}/items/{file_id}/content"

        logging.info(f"[Graph] Downloading é…è»Šè¡¨ (å¤§é˜ª): {name}")
        try:
            r = requests.get(dl_url, headers=headers, stream=True)
            r.raise_for_status()
        except Exception as e:
            body = getattr(r, "text", "")[:200] if "r" in locals() else ""
            logging.error(
                f"[Graph] é…è»Šè¡¨ download failed for {name} (å¤§é˜ª): {e} | body={body}"
            )
            continue

        out_path = dest_root / name
        with open(out_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

        count += 1

    if count == 0:
        logging.warning(
            f"[Graph] é…è»Šè¡¨ not found (no matching Excel with 'é…è»Š') "
            f"for å¤§é˜ª, date '{jp_folder_name}'"
        )
        return None

    logging.info(
        f"[Graph] é…è»Šè¡¨ download finished for å¤§é˜ª â€” {count} file(s) saved under: {dest_root}"
    )
    return dest_root

TOCHIGI_HAISHA_SHARE_URL_TEMPLATE = (
    "https://nskkogyo.sharepoint.com/sites/mouka/"
    "Shared Documents/çœŸå²¡å·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
)

def download_tochigi_haisha_for_date(jp_folder_name: str, base_dir: Path) -> Path | None:
    """
    é…è»Šè¡¨ downloader for æ ƒæœ¨.
    The é…è»Šè¡¨ is directly under the DATE ROOT.
    """
    target_url = TOCHIGI_HAISHA_SHARE_URL_TEMPLATE.format(date=jp_folder_name)

    logging.info(f"[Graph] é…è»Šè¡¨ target SharePoint URL (æ ƒæœ¨): {target_url}")

    share_id = _encode_share_url(target_url)
    list_url = f"{BASE_URL}/shares/{share_id}/driveItem/children"
    headers = {"Authorization": f"Bearer {get_access_token()}"}

    try:
        resp = requests.get(list_url, headers=headers)
        resp.raise_for_status()
    except Exception as e:
        logging.error(f"[Graph] é…è»Šè¡¨ list request failed (æ ƒæœ¨): {e}")
        return None

    items = resp.json().get("value", [])
    if not items:
        logging.warning(f"[Graph] é…è»Šè¡¨ folder empty for æ ƒæœ¨, date '{jp_folder_name}'")
        return None

    dest_root = Path(base_dir) / "é…è»Šè¡¨"
    dest_root.mkdir(parents=True, exist_ok=True)

    count = 0
    for it in items:
        if "file" not in it:
            continue

        name = it.get("name", "")
        lower = name.lower()

        # SAME RULE AS OSAKA â€” å¿…ãš "é…è»Š" + excel 
        if "é…è»Š" not in name:
            continue
        if not (lower.endswith(".xls") or lower.endswith(".xlsx")):
            continue

        drive_id = it["parentReference"]["driveId"]
        file_id = it["id"]
        dl_url = f"{BASE_URL}/drives/{drive_id}/items/{file_id}/content"

        logging.info(f"[Graph] Downloading é…è»Šè¡¨ (æ ƒæœ¨): {name}")
        try:
            r = requests.get(dl_url, headers=headers, stream=True)
            r.raise_for_status()
        except Exception as e:
            logging.error(f"[Graph] é…è»Šè¡¨ download failed for {name} (æ ƒæœ¨): {e}")
            continue

        out_path = dest_root / name
        with open(out_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)

        count += 1

    if count == 0:
        logging.warning(f"[Graph] No é…è»Š Excel found (æ ƒæœ¨) for date '{jp_folder_name}'")
        return None

    logging.info(f"[Graph] é…è»Šè¡¨ download finished for æ ƒæœ¨ â†’ {count} files")
    return dest_root


# ---------------------------------------------------------------------------
# Backward-compatible wrapper (Osaka only)
# ---------------------------------------------------------------------------
def download_osaka_bom_for_date(jp_folder_name: str, base_dir: Path) -> Path | None:
    """
    Legacy helper kept for backward compatibility.

    Internally calls:
        download_factory_bom_for_date("å¤§é˜ª", jp_folder_name, base_dir)
    """
    return download_factory_bom_for_date("å¤§é˜ª", jp_folder_name, base_dir)

def upload_usb_to_tochigi_date(jp_date: str, usb_folder: Path) -> int:
    """
    Uploads the local â–½USB folder contents into:

        çœŸå²¡å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/A
        çœŸå²¡å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/B
        çœŸå²¡å·¥å ´ è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}/C
        ...

    (No USB folder exists in Tochigi â€” truck folders go directly
     into the DATE ROOT.)

    Returns number of uploaded files.
    """

    usb_folder = Path(usb_folder)
    if not usb_folder.exists():
        logging.info(f"[Tochigi Upload] USB folder not found: {usb_folder}")
        return 0

    # --- TARGET ROOT ---
    target_url = (
        "https://nskkogyo.sharepoint.com/sites/mouka/"
        "Shared Documents/çœŸå²¡å·¥å ´ã€€è£½é€ ãƒ‡ãƒ¼ã‚¿/{date}"
    ).format(date=jp_date)

    logging.info(f"[Graph] Tochigi upload target = {target_url}")

    share_id = _encode_share_url(target_url)
    token = get_access_token()
    headers = {"Authorization": f"Bearer {token}"}

    # Resolve DATE folder driveItem
    resp = requests.get(
        f"{GRAPH_BASE_URL}/shares/{share_id}/driveItem",
        headers=headers
    )
    if not resp.ok:
        logging.error(
            f"[Tochigi Upload] Failed to resolve date folder: "
            f"{resp.status_code} {resp.text}"
        )
        return 0

    info = resp.json()
    drive_id = info["parentReference"]["driveId"]
    date_folder_id = info["id"]

    # --- UPLOAD ---
    uploaded = 0

    # Iterate through â–½USB/*  (truck folders)
    for root, dirs, files in os.walk(usb_folder):
        rel_root = Path(root).relative_to(usb_folder)

        for fname in files:
            local_path = Path(root) / fname

            # For Tochigi â€” upload directly under date folder
            # Ex: A/file.pdf â†’ {date}/A/file.pdf
            if rel_root == Path("."):
                # A file directly inside â–½USB (rare but supported)
                remote_rel = fname
            else:
                remote_rel = str(
                    PurePosixPath(rel_root.as_posix()) / fname
                )

            put_url = (
                f"{GRAPH_BASE_URL}/drives/{drive_id}/items/"
                f"{date_folder_id}:/{remote_rel}:/content"
            )

            try:
                with open(local_path, "rb") as f:
                    put_resp = requests.put(put_url, headers=headers, data=f)

                if put_resp.status_code in (200, 201):
                    uploaded += 1
                    logging.info(
                        f"[Tochigi Upload] OK â†’ {remote_rel}"
                    )
                else:
                    logging.error(
                        f"[Tochigi Upload] FAILED {remote_rel}: "
                        f"{put_resp.status_code} {put_resp.text}"
                    )
            except Exception as e:
                logging.error(
                    f"[Tochigi Upload] Exception for {remote_rel}: {e}"
                )

    logging.info(f"[Tochigi Upload] DONE â†’ {uploaded} file(s)")
    return uploaded

